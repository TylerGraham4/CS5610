WEBVTT    OK.    So, umm, so today we're gonna go over linear algebra and geometry    and geometric interpretations to kind of get a little bit more    familiar with some of the operations that we're gonna use    and also start putting you in the mind frame of how.    How we view data and how we organize that data?    But before we do that, I wanted to point out a couple things.    So last meeting I did tell you that I goofed with the the due    date for project two.    I'm still maintaining Project 2 for October 16th.    However, I have.    I did realize that there were errors.    That error was propagated down through Project 3A and 3B.    So because we haven't started on those, I have shifted those due    dates.    Again, if there's an issue with this, contact me outside of this    I I can work with you, but like I was saying before, to stick    with the deadlines and to stay on topic for the projects going    along with the lectures, I think it's important for us to have    these due dates.    So Project 3A right now is gonna be due October 16th, which you    see is the same date as Project 2 being due.    I'm going to keep it that way again, unless you come in, plead    your case with me.    So I would highly encourage you to finish our project too and    get started on Project 3.    So I think at the end of lecture what I'll do is I'll give you an    overview of Project 3 and what you're gonna be doing.    OK.    And introduce them.    OK.    So today we're gonna do a linear algebra.    So I have the slides pulled up here.    I don't know what I changed here.    That's a little worrisome, but we shall get going.    OK.    So again, this lecture is umm for those of you who.    Maybe you haven't seen linear algebra concepts in a while, or    maybe ever.    Umm, I'm not quite sure it was a prereq of this.    Any experience with linear algebra or did anyone take a    linear algebra?    Umm, no.    Many years ago.    All right.    Many, many years ago.    OK, well, stick with me.    Yeah.    I'll try to make this as painless as possible.    Linear algebra.    Algebra is prequel, OK?    Oops.    And I'm already skipping ahead.    OK, so linear algebra we have this concept of vectors, so    here's a vector that we have.    This vector starts at the origin and goes to this point here    which is described AS2 comma 3.    All right, so why is this important?    So I think I introduced last time that we oftentimes think    about or data I copying feature space and we can think of them    as points within feature space.    It is also sometimes useful to not just think of them as    points, but to take this interpretation of them as    vectors within feature space.    And I think by the end of this lecture, hopefully I'll give you    some reasons as to why we might wanna treat them as vectors.    But uh, but we certainly have this interpretation for our data    points.    So here's this vector 2.    Comma three, we have the first dimension.    Uh, first and then the second dimension, right?    So first dimension second.    OK, typically you'll see.    Uh.    Vectors themselves.    Written with angle brackets, variable names are usually in    lower case with an arrow over the top.    So if we're talking about a vector, it's a single dimension,    meaning that it is in array like a one by N array, right?    So these are vectors.    OK, so we can create vectors from two points.    So here we have two points within our space.    Now, I said before that we can think of the interpretation of a    vector or of a point here as a vector with the origin here.    To that point in space.    But sometimes we'll wanna think about vectors in space so we can    do this from 2 points in space.    So here's our .1, which is 2 comma 5 and then we have .2    which is 3 comma one and if we subtract these points from each    other it describes the vector that goes in between them that    starts at 2/5 and goes to three one.    OK, alright.    So not only can we describe points, we can describe them as    vectors.    We can also describe a vector that goes between two points,    OK, and we do that through the the subtraction operation.    OK.    And then this vector.    Here we can describe as also one common -, 4 which we can also    describe.    Here we can conceptualize this as a vector that starts at the    origin, then goes to one common -, 4.    But you can see that the direction and magnitude of this    vector is the same as this one up here.    OK, so that's an important point.    Is that these vectors give us direction as well as overall    length of them.    Alright.    And those are important features that we're going to make use of    when we start applying mathematical models to our    points or our observations.    OK, so we can represent vectors using a library called Numpy.    I think I've mentioned Numpy before.    I don't think we've directly used Numpy, though we've been    relying really on pandas, so pandas itself is built on Numpy,    so you can actually go into pandas and you can return Numpy    arrays or ND arrays.    They call them from a series or from a data frame by using the    DOT values property or calling the dot values properties so    that returns an ND array which is the Numpy data structure.    OK, so vectors are represented as ND arrays and Numpy.    So you can create them from Python lists, so you can use    MPs.    So MPs the what do we wanna call this?    The reference that we use for Numpy itself, so you can do MP    dot array and you can generate an ND array.    From this you can also they have some of these functions in here    for commonly created arrays like.    If you wanted an array of zeros, you could use MPP dot zeros and    it will create a vector of all zeros and then here you can also    create a vector of all ones using NP ones.    OK.    So good number.    OK, so we have this idea of scalar multiplication then, so    vectors can be multiplied by a scalar.    So if you have this vector here, which consists of the values 5,    three and one single dimension, if we come in and with Numpy    define this vector and then use our multiplication operator and    just a scalar value.    So a single value, what it's going to do is it's going to do    element wise multiplication.    So it'll do 5 * 5 three times 5 and 1 * 5, and this is the    resulting vector.    So if you do scalar multiplication with vectors in    Numpy, you get back a vector.    That's the same length and shape, OK.    No biggie.    Alright, we can also do vector addition, so we can do an    addition of 1 vector with another vector.    So in this case we have these two vectors that are each of    length three.    So the 1st 1123, the second one is 531 and if we do addition    with this.    So if we use the addition operator within Python, what it    is going to do is it's going to do elementwise addition.    So what you can appreciate from this is that if you wanna add 2    vectors together, they have to be of the same shape or they    have to have the same dimensions.    OK, so it doesn't make sense to add a four element vector to a    three element vector.    OK, cool.    So that's what that operator will be.    All right, now we have a dot product.    So we are going to use dot products a lot and in fact if    you look carefully, A dot product looks an awful lot like    a model that we've already talked about in this class.    OK.    So that product is a fundamental operation within linear algebra.    So you can perform dot products between two vectors.    So what a dot product between two vectors is going to do again    in this case, if you're doing a dot product between two vectors,    you want the vectors to be of the same length.    They have to have the same number of elements, So what this    does is it multiplies the first two elements together in,    multiplies the second two elements together, and    multiplies the third two elements together, in this case,    and then it takes the sum of all of those.    OK, so you can appreciate unlike scalar multiplication, and    unlike vector addition, what the dot product is going to do is    it's going to take your end length vector or two in length    vectors, and it's going to return a single value from them.    Yeah, arsalon.    So I imagine you're gonna explain this little more about    when you're going through this.    I I know that this is a fundamental concept of finding    weights when you're training out like a model, can you kind of    explain a little bit?    Just give us some hint about what these what these actually    represent in terms of our like training a machine learning    model.    There's like, is there a way to or something?    So we're not quite there yet.    I'd rather I'd rather get through these fundamentals first    and then we can start seeing how we can apply them to models.    Umm.    But I think if you're keen eyed here, this equation here looks    an awful lot like multiple linear regression where we have    a coefficient here and then we have a value of one of our    observations, right?    So this is like coefficient for future one and this is feature    one coefficient for feature two.    This is feature two and so on and so forth.    So you can actually use that products or what we'll see is    matrix multiplication, which is.    A very similar to dot products and we can use these two apply    parameters, 2 models.    We're not going to use this to directly solve for the    parameters.    OK, alright.    So to do this in Numpy, you don't wanna use the    multiplication symbol, you wanna use the method NP dot dot. OK.    Umm.    Groovy.    So there we are.    What about magnitude though?    So I said that vectors themselves not only have    direction, but they have magnitude as well.    Sometimes it's helpful for us to be able to talk about or think    about the magnitude of a vector.    So the way that we can calculate the magnitude of the vector is    we can take the dot product of a vector with itself and then take    the square root of this.    So what is this doing?    This is multiplying the values of each one of its dimensions.    It adds them all up and then it takes the square root of it.    OK, sounds good.    So to do this, you can layer these together.    So here's our vector of 1111.    So this is a four dimensional or it we could think of it as an    observation described by 4 features and here you can see    that we can dot it with itself and then take the square root of    it and we get two.    So the magnitude of this vector is actually two OK.    All right.    So not only are magnitudes important, but a very, very    common metric that a lot of machine learning models make use    of.    Is this idea of distance so Euclidean distance?    You may have heard of this before, but why is this    important?    So oftentimes what we're curious about within our feature space    is how does one point relate to another point?    And one way that we can describe that relationship is what is the    distance between these points?    So you can imagine that observations that are described    by very similar values of features are going to be very    close to each other in Euclidean distance, and two observations    that are described by very different feature values are    going to be far away from one another.    So that's why we get why we use Euclidean distance.    So we can calculate the distance by uh using this expression.    So this expression what you're doing here is you're really    taking the dot product between your vector U minus your vector    V and itself again, and then take the square root of that.    OK.    With me.    OK, So what this comes out to is you take the first feature, the    first element of you minus the first element in TV.    You take that difference, you square it and then you take all    of these differences, square them, add them all together and    get the square root.    That'll give you Euclidian distance, OK.    And in fact, if we take a look back here at magnitude, this is    the exact same thing that magnitudes doing.    It's just we can imagine a -, 0 here, right, because we're    taking the magnitude to this point from the origin.    OK, so that gives us the magnitude Ruby, but.    All right, so Euclidian distance.    So magnitude and direction vectors have magnitudes, lengths    as well as direction, so the magnitude as we talked about    here we can solve for the Euclidean distance between the    origin and this point here.    And to get direction what we can do is we can take the vector and    then divide by this magnitude. OK.    All.    So in addition to these basic functions, let's take a look at    that product a little bit more in depth and trying to like    build an intuitive understanding of what is the dot product    actually doing in our space.    So there are a couple of properties here.    So one property is that products compare vectors, right?    So what we can get out of this is if we have two vectors and    these vectors are orthogonal to one another, so they're at in    two dimensional space, they're at right angles from one    another.    If we evaluate the dot product on that, we'll actually get a    dot product of 0.    So just by evaluating the DOT product on any two vectors, if    it comes out at zero, then we know that these vectors are    orthogonal to one another.    OK.    Uh.    In a similar idea, if we have two vectors here, but these    vectors are going in the same direction, if we take the    normalized dot product or or the dot product of the unit vectors,    what we'll get is the dot product between these two.    If they're going in the same direction and they're parallel,    the dot product equals one.    And if there are anti parallel to one another, this normalized    bad product is -, 1 so one way that I like to view dot products    and you'll see them all over the place.    11 way that I like to view that products is they're kind of like    a compass for us within feature space.    So if we have a reference vector that we know a lot about and we    wanna compare another vector to it, it gives us the ability to    compare that vector.    So in two dimensional space we can just plot them and look at    them.    What happens when we get to like 50 dimensional space?    The DOT product still applies, right?    We can expand this out to many different dimensions, but it    acts as a compass to tell us like, how does this other vector    compare to the one that we're interested in? OK.    Can you go back to?    Alright.    Sorry, can you go back to real quick or are they all unit    inspector or this one this does not need to be?    Umm so.    So I do unit vector, so if you do the unit vector, if you    normalize the vector by its magnitude and then you do this    comparison if they're parallel to one another, it will be one.    If they're not parallel to one another, it'll be a positive    value, but.    Yeah.    Right.    It won't be one and -, 1 and similarly here.    But the purpose?    OK.    Yeah, but in the perfect.    So that that's for the parallel and antiparallel, but in the    perpendicular case.    You don't need to do unit vector stuff.    It's just you just stop them.    Yeah, this will always be true.    OK. Thanks.    OK.    Yeah.    And in fact, if you know, I'll do the thing as Professor quote.    If you wanna prove this to yourself, pick two vectors that    you know are orthogonal to one another and do the dot product    and you'll see why it always comes out as zero.    OK.    All right, so normal vectors.    OK.    So normal vectors are perpendicular to surfaces, so    that's just what a normal vector is.    OK, so if we have a surface, we can define a normal vector for    that surface, and that normal vector he is any vector that    meets this.    Uh.    Condition in that is perpendicular to the surface    itself.    So here you see this plane that's being visualized.    And here are two normal vectors to this plane, and what you can    see is that these normal vectors themselves are perpendicular and    coming directly off that plane off that surface.    OK, so normal vectors are often used for defining geometric    object or indicate its origin itself.    So remember, last time I was talking about how we think about    classification and decision boundaries, right?    And I was also talking about hyperplanes.    I mentioned hyperplanes.    So hyperplane, so a plane is a flat geometric object that    divides the space in half.    Umm.    So if we have a space with end dimensions, a plane is going to    be an n -, 1 dimensional object, and that's known as a    hyperplane.    In general, it's known as a hyperplane, so this might be a    line in two dimensions.    It might be a plane that we understand in three dimensions,    or it could be a hyperplane.    If we go to four or five 6710, a 100 dimensions.    OK, So what you can see here is we can define this plane using a    point on the plane and a normal vector, so that is one way that    we can define planes themselves that go through spaces is    through just a single point that resides on that plane and a    normal vector.    OK, so planes are defined.    So this is the normal form using a normal vector N and a point    are not that lies on the plane.    OK, so all points are for which the equation is true lie on the    plane and all points that are not.    Sorry, all other points are not on the plane, So what this means    is we can define our not right.    We can define a normal, uh vector and then given these two    all this constant, these are going to define a plane in space    for us and this is related through this expression.    So what does this expression doing?    Well, I just told you earlier that if we take two points and    we subtract them, we got the vector that spans in between    them, right?    So if I have this point P on my plane and I or we could think of    this as R not and I have another point that's on my plane.    If I subtract these two, I'm going to get a vector that rides    along.    That's in line with my plane, right?    You'll see that.    So that's going to describe a, A, an angle or direction and    magnitude.    And then what this relationship says is I also told you that if    we take a vector and we dot it with another vector which this    vector rides along the plane, I said that this is always going    to equal 0 for for what?    Perpendicular vectors.    This is grade class, maybe I shouldn't ask active questions.    What was that?    Not for perpendicular vectors.    Yeah, so perpendicular.    So this is a vector that rides on your plane.    This is a vector.    That's normal to your plane, so these two together is going to    equal 0 because they're perpendicular to each other,    right?    The normal vector is going to be perpendicular to the vector    that's on your plane.    OK.    So that's where that comes from.    So the equation measures the angle between the normal and the    factors.    If the two vectors are orthogonal, then they're not    product is all right.    So checking we're point lies relative to a plane, so we can    use a hyperplane to divide a space in half, and knowing that    the dot product measures angles, we can use the definition of a    hyperplane to determine on which side of the plane to point lies.    This is really kind of cool, right?    So we have this condition which can help us find points on the    plane.    But if we substitute this out where this points on the plane,    this normal vector is normal to the plane and we start    substituting points in here what we will find is that if the    point ends if the if, this evaluates out to a positive    value, then it's going to be on one side of the plane.    If this evaluates out to a negative value, it's going to be    on the other side of the plane, and if it evaluates out to zero    then it's going to ride right along the plane, and this forms    the basis for a lot of decision.    Wow, this forms the basis for logistic regression, at least in    that if we can describe these two things we're describing the    hyperplane, the hyperplanes, the model, and then by applying this    model to new points, we can tell which side of the plane a point    actually resides on, or which class to classify it as you    digging.    Alright, that's great.    Well, is the question. Umm.    Wait, is this recent Rees?    And the main.    Uh, yeah.    I was just talking about like the math thematical explanation    for why it always works out to be 0 uh in the chat.    Ohh yeah yeah, you do have the the relationship to cosign for    dot products as well.    OK, alright, cool.    So, oh, so this is what this is saying.    So if you have your normal vector and your point on the    plane, and you evaluate R and it's equal to 0, the point lies    on the plane.    Otherwise, if this is greater than zero, then the point lies    on the same side of the plane as the normal vector, and if this    is negative, then the point lies on the opposite side of the    plane as the normal vector.    This is also neat.    Well, I think I think I have that later.    OK, alright.    That makes sense to everyone.    Everyone driving with me.    OK, alright.    So let's now talk.    Those were vectors.    Let's talk a little bit about matrices.    So matrices are ordered collections of vectors, so    that's a great way to think of matrices.    So we can have either column vectors or row vectors in a    matrix, right?    So here we have column vectors and.    These are row vectors.    OK, so this matrix is made up of three row vectors.    That's one way of thinking about uh, and they're used to store    and perform operations on groups of vectors in a single    operation.    So I think you can appreciate that we have feature matrices    that what we're doing in pandas is we're creating data frames    and then what we can do with Numpy is if we can get all of    these values in our data frame into the same data type, we can    make a feature matrix out of it.    OK.    And uh, our interpretation of matrices follows that in that    each row is going to be an observation, and that each    column is going to be a new feature or a new dimension.    OK, alright, so here are three points that are matrix actually    defines.    So in our interpretation of this matrix, we can see that we have    three points, one point at 131 point at 5 seven and one point    at 911, and you see them all in a line right here.    OK.    It just so happens that these end up being in a line.    There's nothing requiring that, though.    OK, so each row represents a point and each column then    represents a dimension or the value along a given axis.    Alright, so how we represent matrices?    So a variable names are with a capital letter typically, and    then we have an arrow over the top.    So matrices use lower case in an error.    Excuse me?    Vectors use lowercase and an arrow matrices use uppercase and    an arrow.    OK, matrices are described by the number of rows that they    have.    The number of columns they have and the type of number, so you    might see something like this where this is describing we have    matrix a here which is made up of real valued numbers and it's    dimensions are 5 rows and three columns.    Or we have 5 observations in this described by the same 3    features.    OK.    All right can be described using.    Ohh, we can also describe them using indices directly.    So if I wanted to actually refer to, uh, a vector in here, I    could do that.    I could also refer to a single element 2 as described by its.    In this case row and column OK, so it's always row, then column    or rise and then run.    OK, alright.    So we can use and be arrays in Numpy to not only describe    vectors, but we can also use them to describe matrices.    So to do that, we're going to use are nested listed nested    lists.    So here we have three lists within a larger list, so each    element in this larger list is made up of a list, and in this    case it has two values.    All right.    So if we do this, we're going to get a shape and call the shape    property on the ND array.    The shape is gonna tell us that it's three comma two.    Once you start working with matrices in Numpy at all, really    really rely on shape, I use shape all the time because what    we'll see is a lot of matrix operations depend on the shape    of the matrix itself and operations will fail if you    don't meet the required shape.    I guess you can think of it that way.    OK.    And similarly to creating vectors, we can use methods like    ones and zeros and provide the dimensions that we have in here.    So something to keep in mind here, there's a gotcha that I    want you to be aware of is that Numpy, for whatever reason, can    use a.    You can define a ND array as having a single dimension, so    that's what I showed you before.    So you can describe an empty array as vector of three    elements long.    That is actually different.    It's a different representation than another option that this    doesn't immediately show you, but another option is you could    describe a vector as three comma one, so it's going to be thought    of as a matrix in ohm in Numpy, but it is going to have just    elements along a single dimension.    Does that make sense?    So wrong and depending on the operation that you're doing,    some of them are expecting like if you trying to mix and match    like you define one vector as a Singleton.    Dimension and you describe another vector as being like 3    comma, one.    Those have different interpretations.    OK, so they don't always jive.    So if you start to see things that are really weird and    failing you shape to figure out what is actually interpreting    the dimensions as and make sure that you're consistent with    those.    I like in the way that I think about it.    Is, I think about everything as these matrices, so I would add a    like a three comma one if I wanted to represent a vector,    but also appreciate that A3 comma one factor is going to be    in this case it would be a column vector and A1 comma three    is going to be a row vector.    OK.    You driving, OK?    Doesn't I?    We have a number of operations that we can perform with Numpy,    so the way I think about Numpy is Numpy is really a linear    algebra library for Python, so transposing so transposing a    common ohm operation, transposing what it does is it    rotates the matrix around the diagonal so that if we transpose    it, we're going to flip all the elements in the matrix across    the diagonal.    So what that means is if you have a square matrix, so if you    have a matrix that's two by two or four by four, it's going to    keep the diagonal the same, and it's going to flip all of the    things in the top triangle into the bottom, and vice versa.    That means if you have a non square matrix like a three comma    two then what is going to do is it's going to make it into a 2    comma three matrix.    OK.    So it actually flips those dimensions.    So to transpose things here we have this three comma two.    You can use the T the T is the transpose or calls transpose on    the matrix, and in this case we transpose.    A B then returns to row vectors that is 159 and three 711 and    its shape is now 2.    Three, OK, alright, now operations on matrices.    So we have now a matrix and we can dot a matrix with a vector    and this is going to give us a vector.    Ohh arsalon I didn't see your hand.    I don't know how long your hands.    Don't.    What's up?    Uh, I just rose it on the last slide.    I've seen this idea of transposing matrices, either in    a preprocessing step.    I think it was prepro data preprocessing.    Why?    Why do we do that?    I think there are various reasons why we might wanna do    that.    If you wanna look at features.    For us, one reason that we might want to do it as if we wanna    look at features versus observations, like if we wanna    flip that information, it depends on the representation    that you wanna use.    And if you wanna calculate things like covariance matrices,    what you can do is you can use the transpose multiplied by the    matrix itself and what that effectively does is it will it's    leading to a a covariance matrix which tells you how does one    feature actually vary with another feature.    OK, so multiplication so we can do dot products between a matrix    and a vector, but we have to adhere to a certain shape and    standard.    So what you get out of this is another vector, and this is    where the shapes starts to matter.    So in this case this is going to be a 3 comma 2SO3 rows by two    columns and this is A2 rows by one column, right?    So the inner dimensions have to match.    So what this is effectively doing is it's doing the dot    product on these three observations with this vector    here.    So this takes 1 * 1 + 3 times -, 1, and we get out here in this    element -, 2, five times 1 + 7 times -, 1 we get, -, 2, and so    on and so forth.    OK, so the resulting, umm shape then of this vector is going to    be the outside dimension.    So this is a 3 comma two and this is a 2 comma one, so the    inside dimensions must match to be able to perform this    operation and then you get a vector.    That's the outside dimension.    So in this case two and two match.    So we could do it and then we have a 3 comma one, OK.    Alright, if those rules do not match you, it is an illegal    operation.    Cannot perform that and you will get goofed and Python will    complain about it.    OK, so check their ships.    Umm.    All right, so the length of the vector must match the number of    columns in the matrix.    That's what I just said.    OK.    We also support multiplication on matrices, so this works in a    very similar way, so that dimensional rule still applies.    So here we have a three comments, two and this is a 2    comma two so we can perform this.    It effectively performs sequential dot products though,    so it'll be the dot product between this row vector and this    column vector will be then the dot product between this row    vector and this column vector.    So that's the first dot product.    That's the second one.    This row now and this column, this row, this column so on and    so forth.    You do so again.    That rule applies.    This is a 3 comma, 22 comma, 2.    The inside dimensions match two and two match and then we get a    resulting vector.    That's three kind of two.    OK.    The number of columns in the first matrix must match the    number of rows in the second to perform.    So the way that we can do this is here we have matrix vector    multiplication.    So you can use the dot product between a matrix and a vector    and that will return this array here and then for matrix    multiplication you can actually use dot as well.    All right, you can also use Matt Mull, so NP dot Matt Mull also    does matrix multiplication.    OK, so interpreting matrix multiplication, why is this    important?    So it's important for a number of reasons.    So one Model, 1 mental model that you can use for application    of a matrix onto another matrix is transformation.    So a lot of times you could think of if you're doing matrix    multiplication between a matrix and a vector, you're going to be    transforming this vector in some way based off of these    parameters.    So the best way to conceive of it are conceptualized.    This is what we can end up doing is we can end up forming our    multiple linear regression model in the form of linear algebra    where the coefficients of our model is a vector and then the    data that we're applying it to is a matrix, our feature matrix.    So you can do matrix multiplication between these two    and what it ends up with is that ends up with your response,    right?    So in in your project, you're solving for multiple linear    regression on your data set, right?    And how many features do you have in your data set?    I don't remember.    Sorry, I'm asking open ended questions that doesn't really    work in this for me.    Ohh, I was teaching earlier today in person.    Umm, So what was I saying?    So you might have 10 features, right?    You have beds, baths, square footage, latitude, longitude.    You have all of those.    You apply your multiple linear regression model to it and then    it transforms that into a single value, which is your price.    Your response right?    So it's taking some kind of transformation.    It's applying it to your matrix, which is your feature matrix and    what you get out is you get a vector out that is a    transformation of all of your data into the response.    That's the same thing that matrix multiplication is doing.    Umm, one of the reasons that we do it this way is that it we    have really efficient libraries that are I think compiled and    see for use with Numpy.    So it's really, really good at doing like matrix multiplication    when we have things formatted this one.    OK, so conversion other things that we can use it for is    conversion from one coordinate system space to another.    So I'll show you in a moment with PCA what we do, what we    solve with PCA is that we can solve for a transformation    matrix and then we can apply that to our data and we can    transform our data from its original feature space into the    component feature space.    All right, that's another application we can project into    lower dimensional space.    That's what I was saying as far as taking your features and    resolving them as a single response.    That's a transformation into a lower dimensional space.    You're going from 10 features down to a single value, and we    can project them on to a space onto again space.    Yeah, arson.    Umm can you go back to the previous slide and show us how    we get a multidimensional array in the dot product has a single    value as its output.    It seems here that the output is actually still has multiple    columns and rows.    Right.    So with the dot product you can think of matrix multiplication    as a special application or a repeated application of dot    products.    So umm, Numpy supports the overloading of I?    Maybe it's not even overloading.    Yeah, I guess it is of the dot method.    So this is really performing matrix multiplication between a    matrix in this case and a vector.    So because of that, it's still adheres to these rules where we    use the dimensions themselves, in this case A3 by two and a two    by two, and we get a three by two out.    It's just that Numpy itself supports the use of dot for that    OK.    as well as uh, Matt Mull.    OK. Thanks.    Sure.    OK, so one way of thinking about this is that we can think about    matrix multiplication.    So there's this example in here if you're more familiar with    Fourier transforms.    Umm.    Uh, Fourier transforms.    What do they do?    They decompose your signal, right?    Or really what they're doing is they're transforming it into a    different space.    So that's where this analogy comes from.    For those eyes out there, maybe this is a better way of thinking    about it, and that if you have a signal in time what the Fourier    transform does is it actually turns that to a signal in    frequency.    So you're changing between these two.    So that's one one way to our conceptualize it right and here    you can see that we have our frequency values.    We have this matrix over here which this matrix would describe    our different individual sinusoids that we're trying to    decompose the signal into, and then we have matrix    multiplication with the data in the time domain, and we can get    this extraction out this transformation.    OK.    So we're all talking about the same thing.    These are just different ways of conceptualizing it.    I think once you start using matrix multiplication, it can be    confusing at first, but I just want to give you a number of    examples that may be one of these strikes you.    Well, OK, so here's this example we talked about principal    component analysis before.    So what we can do is we can actually solve in principal    component analysis for this matrix.    This matrix over here and then by doing matrix multiplication    between our feature matrix in our original feature space and    this solve matrix, we can transform the data into the    principal components into a lower dimensional space. OK.    Just a few more things and then I think we're just about done.    So we also have identity matrices, so these are just kind    of like fun special forms of matrices.    So the identity matrix is a matrix with zeros on the off    diagonals and then ones down the diagonal.    So with the identity matrix, does is that if you have a    matrix and you multiply it by the identity matrix, you get    that same matrix out.    OK.    We also have a negation matrix, so if we wanted to negate all    the values in our matrix, we could use a identity matrix    multiplied by -, 1 by the scalar value -, 1 OK that we can get a    negation.    Ohm.    Why would we need the normal identity matrix?    I think there are a number of reasons that we would.    One way that it appears is that if we're doing some kind of    algebraic manipulation, so if we're using linear algebra and    we wanna switch like a matrix from one side of the equation to    the other side, what we can do is we can take the inverse of    the matrix.    Wow, inverse of the matrix is not always guaranteed, but if we    can take the inverse of the matrix, the inverse of a matrix    matrix multiplied by the matrix itself returns the identity    matrix.    So it's effectively like multiplying by 1.    OK.    And in fact, you remember the normal equation.    OK.    I'll go back to that.    That's one of the solutions for multiple linear regression to    find the parameters.    I'll go back to the I'll pull that up and after hours and    I'll, I'll show you what is actually going on.    The OK we can also define a permutation matrix.    So what the permutation matrix does is it swaps the values    within the columns, or it swaps the columns.    So in this case we have 1357911.    If we do ones off the diagonals, what we end up with is 3175119.    Umm.    And then we can do operations together.    So if we wanted to, you negate as well as reverse the ordering,    we could apply these together.    So one thing that I hope you get away from this lecture is that    the order that matrix multiplication happens in does    in fact matter.    So it it's, uh, what's the property?    Uh, commutative.    It doesn't support the commutative wait.    I'm gonna embarrass myself.    I'm not gonna tell you that.    Scratch that from the record.    Uh, delete that back up 10 seconds and then or Fast forward    10 seconds and then play the video OK.    So we can combine these together.    And then yeah, I think that was the last of what I wanted to    cover today.    Any questions over that?    Let me pull up the normal equation.    We can take a look at that again.    But while we're doing that, any questions?