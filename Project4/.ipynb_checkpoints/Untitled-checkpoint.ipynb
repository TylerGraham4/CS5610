{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b80e91d-af04-46fc-b49d-d03d886614b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ba72be-c118-4ecf-91e5-67beefb02242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'kc_house_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f197ff-2e4d-46c3-bd40-c791419b48f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is any nulls\n",
    "null_counts = data.isna().sum()\n",
    "print(\"Number of Null itmes in each column: \", null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f2b41c-16ba-4cee-b231-531390eb7b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data' is your DataFrame with 'id' and 'price' columns\n",
    "# Step 1: Calculate the average price per ID\n",
    "average_prices = data.groupby('id')['price'].mean().reset_index()\n",
    "\n",
    "# Step 2: Merge the average prices with the original DataFrame\n",
    "merged_data = data.merge(average_prices, on='id', suffixes=('', '_avg'))\n",
    "\n",
    "# Step 3: Replace the original price with the averaged price\n",
    "merged_data['price'] = merged_data['price_avg']\n",
    "merged_data.drop(columns=['price_avg'], inplace=True)\n",
    "\n",
    "# Step 4: Drop duplicate IDs\n",
    "final_data = merged_data.drop_duplicates(subset='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22186171-d063-40c3-95aa-2e96f54d92c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "for column in final_data.columns:\n",
    "    duplicates = final_data[column].duplicated().any()\n",
    "    print(f\"Column '{column}' has duplicates: {duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a2a0c9-2ea0-42a8-9dd1-54ceabdf3e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_bins = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]  # Adjust this list based on your data\n",
    "for bins in possible_bins:\n",
    "    bin_edges = np.histogram_bin_edges(final_data['price'], bins=bins)\n",
    "    sns.histplot(final_data['price'], bins=bin_edges, kde=True)\n",
    "    plt.title(f'Histogram with {bins} bins')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a162e777-ad2a-4aad-9708-825bf3ff6a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_bins = 60  # Example value\n",
    "bin_edges = np.histogram_bin_edges(final_data['price'], bins=selected_bins)\n",
    "final_data.loc[:, 'bin'] = np.digitize(final_data['price'], bins=bin_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab668bbb-eeb4-460a-94a9-8901515cf954",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data = final_data['price'].values\n",
    "hist_bins = np.histogram_bin_edges(price_data, bins=50)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(price_data, bins=hist_bins, kde=True)\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Home Prices')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf1b51d-2bbc-4ebd-95ed-da4faa62b381",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['waterfront', 'condition']\n",
    "\n",
    "# Create dummy variables\n",
    "final_data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa77fd93-60d8-497e-a449-2002385422d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = final_data[final_data['price'].apply(lambda x: random.random() < 0.75)]\n",
    "set2 = final_data[final_data['price'].apply(lambda x: random.random() >= 0.75)]\n",
    "print(f'Set 1 size: {len(set1)} records')\n",
    "print(f'Set 2 size: {len(set2)} records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08664d6d-fb39-4210-aa4d-798161b8281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'view', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'lat', 'long']\n",
    "\n",
    "# Create a StandardScaler and fit it to 'set1' for the numeric columns\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(set1[numeric_columns])\n",
    "\n",
    "# Transform both 'set1' and 'set2' using the same scaler for numeric columns\n",
    "set1_scaled = set1.copy()\n",
    "set1_scaled[numeric_columns] = scaler.transform(set1[numeric_columns])\n",
    "\n",
    "set2_scaled = set2.copy()\n",
    "set2_scaled[numeric_columns] = scaler.transform(set2[numeric_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf9bcc2-230f-49d1-8a1e-c6d1ad741125",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric_columns = ['date']  # Add any other non-numeric columns as needed\n",
    "X_train = set1.drop(columns=['price'] + non_numeric_columns)\n",
    "y_train = set1['price']\n",
    "\n",
    "X_test = set2.drop(columns=['price'] + non_numeric_columns)\n",
    "y_test = set2['price']\n",
    "\n",
    "# Train a linear regression model on set1\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict prices for set2\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate RMSE (Root Mean Squared Error)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "\n",
    "# Plot predicted prices against true prices for set2\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.xlabel('True Prices (Set2)')\n",
    "plt.ylabel('Predicted Prices (Set2)')\n",
    "plt.title('Predicted Prices vs. True Prices (Set2)')\n",
    "plt.show()\n",
    "\n",
    "# Print RMSE and MAPE\n",
    "print(f'RMSE: {rmse:.2f}')\n",
    "print(f'MAPE: {mape:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b4c106-5773-4307-93f7-c6cafd796baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'set1' and 'set2' are your DataFrames\n",
    "# Identify and exclude non-numeric columns\n",
    "non_numeric_columns = ['date']  # Add any other non-numeric columns as needed\n",
    "\n",
    "X_train = set1.drop(columns=['price'] + non_numeric_columns)\n",
    "y_train = set1['price']\n",
    "\n",
    "X_test = set2.drop(columns=['price'] + non_numeric_columns)\n",
    "y_test = set2['price']\n",
    "\n",
    "# Standardize the numeric features (scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create a dictionary of hyperparameters to search\n",
    "param_grid = {\n",
    "    'loss': ['squared_loss', 'huber'],\n",
    "    'alpha': [0.001, 0.01],\n",
    "    'epsilon': [0.1, 0.2, 0.3],\n",
    "}\n",
    "\n",
    "# Create the SGDRegressor model with increased max_iter\n",
    "sgd_regressor = SGDRegressor(max_iter=1000, random_state=42)\n",
    "\n",
    "# Perform GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=sgd_regressor, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best Hyperparameters: {best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453cc9d7-045f-4e75-adc9-9d5f7ad7fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model with the best hyperparameters\n",
    "best_sgd_regressor = SGDRegressor(max_iter=5000, random_state=42, **best_params)\n",
    "\n",
    "# Train the model with the best hyperparameters on set1\n",
    "best_sgd_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict prices for set2\n",
    "y_pred = best_sgd_regressor.predict(X_test_scaled)\n",
    "\n",
    "# Calculate RMSE (Root Mean Squared Error)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "\n",
    "# Plot predicted prices against true prices for set2\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.xlabel('True Prices (Set2)')\n",
    "plt.ylabel('Predicted Prices (Set2)')\n",
    "plt.title('Predicted Prices vs. True Prices (Set2)')\n",
    "plt.show()\n",
    "\n",
    "# Print best hyperparameters, RMSE, and MAPE\n",
    "print(f'Best Hyperparameters: {best_params}')\n",
    "print(f'RMSE: {rmse:.2f}')\n",
    "print(f'MAPE: {mape:.2f}%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
